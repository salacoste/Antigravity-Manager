# Story-003-11: Tool Mode Testing and Feature Parity Validation

**Story ID**: Story-003-11
**Epic**: [Epic-003](../epics/Epic-003-Claude-4.5-Sonnet-Thinking-Compliance.md) - Claude 4.5 Sonnet Thinking - 100% Compliance
**Priority**: P2 (High - Testing)
**Estimated Effort**: 2 hours
**Status**: âš ï¸ BLOCKED (Story-003-10 NOT IMPLEMENTED)
**Blocker**: Story-003-10 geminiSettings implementation required
**Created**: 2026-01-10
**Updated**: 2026-01-11 (Fixed false assumption about Story-003-10)
**Owner**: Engineering Team

---

## User Story

**As a** developer completing Epic-003 Phase 3 implementation
**I want** comprehensive end-to-end testing of tool modes and grounding configuration
**So that** I can confidently validate 100% feature parity and compliance with Antigravity baseline

---

## Context

### Testing Scope

This story validates **Phase 3 (Feature Parity)** implementation:

**Story-003-09** (Flexible Tool Configuration Modes):
- âœ… ToolChoice enum with 4 variants (AUTO, ANY, NONE, Tool{name})
- âœ… Dynamic mode selection in request mapper
- âœ… allowedFunctionNames for tool forcing

**Story-003-10** (Grounding Configuration):
- âŒ geminiSettings.recitationPolicy NOT IMPLEMENTED
- âŒ action="BLOCK", threshold="LOW" NOT IMPLEMENTED
- âš ï¸ **CRITICAL**: Story-003-10 must be implemented before this story

**Compliance Target**: 95% â†’ **100%** (blocked by Story-003-10)

### Why Comprehensive Testing is Critical

**Anti-Detection Validation**:
1. **Tool Modes**: Ensure all 4 modes match Gemini Protocol exactly
2. **geminiSettings**: Verify always present (fingerprinting prevention)
3. **Request Structure**: Confirm indistinguishable from Antigravity
4. **Backend Responses**: Validate upstream API accepts our requests

**Regression Prevention**:
1. **Backward Compatibility**: Existing workflows must work unchanged
2. **No Breaking Changes**: Previous stories' features must remain functional
3. **Performance**: No degradation from new features

**Compliance Verification**:
1. **Gap Closure**: Validate all 8 gaps from Gap Analysis are resolved
2. **100% Compliance**: Confirm matching Antigravity baseline exactly
3. **Manual Validation**: Live endpoint testing against RE specification

### Current Situation

**Implementation Status**:
- âœ… **Story-003-09**: ToolChoice enum and mode mapping IMPLEMENTED
- âŒ **Story-003-10**: geminiSettings NOT IMPLEMENTED (CRITICAL BLOCKER)

**Testing Needed** (BLOCKED until Story-003-10 complete):
- âŒ No end-to-end validation of tool modes
- âŒ No integration testing with live Vertex AI
- âŒ No compliance verification against RE spec
- âŒ No manual testing scenarios documented

**Risk Without Story-003-10 Implementation**:
- ğŸš¨ **BLOCKER**: geminiSettings missing from all requests
- ğŸš¨ **CRITICAL**: Anti-detection fingerprint (requests identifiable as non-Antigravity)
- âš ï¸ Compliance score stuck at 95% (cannot reach 100%)
- âš ï¸ Testing Story-003-11 is premature without Story-003-10

---

## Reference Documentation

### Primary Dependencies
- **Story-003-09**: `docs/stories/Story-003-09-flexible-tool-configuration-modes.md`
  - Lines 751-1022: Testing Strategy (9 unit + 2 integration tests)
  - Lines 200-515: Implementation details for each mode

- **Story-003-10**: `docs/stories/Story-003-10-grounding-configuration.md`
  - Lines 460-640: Testing Strategy (5 unit + 1 integration test)
  - Lines 150-280: geminiSettings implementation

### Gap Analysis Reference
- **Document**: `docs/comparison/claude/claude-4-5-sonnet/current-implementation-thinking.md`
  - Lines 3730-3806: Testing Strategy and Compliance Verification
  - Lines 3797-3805: Compliance checklist

### Epic Success Criteria
- **Epic-003**: `docs/epics/Epic-003-Claude-4.5-Sonnet-Thinking-Compliance.md`
  - Lines 140-143: Phase 3 success criteria
  - Lines 232-260: Epic-level Definition of Done

---

## Technical Details

### Test Architecture

**Three-Layer Testing Strategy**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Unit Tests (Already in Stories 009, 010)           â”‚
â”‚ âœ… ToolChoice enum                                           â”‚
â”‚ âœ… Mode mapping logic                                        â”‚
â”‚ âœ… geminiSettings structure                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Integration Tests (This Story)                     â”‚
â”‚ ğŸ†• End-to-end tool mode workflows                           â”‚
â”‚ ğŸ†• Live Vertex AI endpoint testing                          â”‚
â”‚ ğŸ†• Request/response validation                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Manual Validation (This Story)                     â”‚
â”‚ ğŸ†• Compliance verification against RE spec                  â”‚
â”‚ ğŸ†• Backend behavior validation                              â”‚
â”‚ ğŸ†• Real-world scenario testing                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Test Scenarios

#### Scenario 1: AUTO Mode End-to-End

**Goal**: Verify model autonomously decides when to use tools

**Test Case**:
```rust
#[tokio::test]
async fn test_e2e_tool_mode_auto() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{
            "role": "user",
            "content": "What's the weather in Tokyo? Use the weather tool if available."
        }],
        "tools": [{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get current weather for a location",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "location": {"type": "string"}
                    },
                    "required": ["location"]
                }
            }
        }],
        "tool_choice": {"type": "auto"},
        "max_tokens": 1024
    });

    let response = send_claude_request(request).await.unwrap();

    // Verify toolConfig in upstream request
    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["mode"],
        "AUTO"
    );

    // Verify model could use tool (or not, at its discretion)
    assert!(response.is_ok());
}
```

#### Scenario 2: ANY Mode End-to-End

**Goal**: Verify model MUST call at least one tool

**Test Case**:
```rust
#[tokio::test]
async fn test_e2e_tool_mode_any() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{
            "role": "user",
            "content": "Get structured data using the tool"
        }],
        "tools": [{
            "type": "function",
            "function": {
                "name": "get_data",
                "description": "Get structured data",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {"type": "string"}
                    }
                }
            }
        }],
        "tool_choice": {"type": "any"},
        "max_tokens": 1024
    });

    let response = send_claude_request(request).await.unwrap();

    // Verify toolConfig.mode = "ANY"
    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["mode"],
        "ANY"
    );

    // Verify model called at least one tool
    let response_content = response["content"].as_array().unwrap();
    let has_tool_use = response_content.iter().any(|block| {
        block["type"] == "tool_use"
    });
    assert!(has_tool_use, "Model must call at least one tool with ANY mode");
}
```

#### Scenario 3: NONE Mode End-to-End

**Goal**: Verify tools are completely disabled

**Test Case**:
```rust
#[tokio::test]
async fn test_e2e_tool_mode_none() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{
            "role": "user",
            "content": "Tell me about the weather (do NOT use tools)"
        }],
        "tools": [{
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Get weather",
                "parameters": {
                    "type": "object",
                    "properties": {}
                }
            }
        }],
        "tool_choice": {"type": "none"},
        "max_tokens": 1024
    });

    let response = send_claude_request(request).await.unwrap();

    // Verify toolConfig.mode = "NONE"
    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["mode"],
        "NONE"
    );

    // Verify model did NOT call any tools
    let response_content = response["content"].as_array().unwrap();
    let has_tool_use = response_content.iter().any(|block| {
        block["type"] == "tool_use"
    });
    assert!(!has_tool_use, "Model must NOT call tools with NONE mode");
}
```

#### Scenario 4: Tool Forcing (VALIDATED + allowedFunctionNames)

**Goal**: Verify model calls only the specified tool

**Test Case**:
```rust
#[tokio::test]
async fn test_e2e_tool_forcing() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{
            "role": "user",
            "content": "Get me some data"
        }],
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather",
                    "parameters": {"type": "object", "properties": {}}
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_stock_price",
                    "description": "Get stock price",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ],
        "tool_choice": {
            "type": "tool",
            "name": "get_stock_price"
        },
        "max_tokens": 1024
    });

    let response = send_claude_request(request).await.unwrap();

    // Verify toolConfig
    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["mode"],
        "VALIDATED"
    );
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["allowedFunctionNames"],
        json!(["get_stock_price"])
    );

    // Verify model called ONLY get_stock_price
    let response_content = response["content"].as_array().unwrap();
    let tool_calls: Vec<&str> = response_content.iter()
        .filter_map(|block| {
            if block["type"] == "tool_use" {
                block["name"].as_str()
            } else {
                None
            }
        })
        .collect();

    assert_eq!(tool_calls.len(), 1);
    assert_eq!(tool_calls[0], "get_stock_price");
}
```

#### Scenario 5: geminiSettings Presence Validation

**Goal**: Verify geminiSettings always present regardless of configuration

**Test Cases**:
```rust
#[tokio::test]
async fn test_gemini_settings_always_present() {
    // Test 1: Request with AUTO mode
    let req1 = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "tools": [/* ... */],
        "tool_choice": {"type": "auto"},
        "max_tokens": 1024
    });
    let _response1 = send_claude_request(req1).await.unwrap();
    let captured1 = get_last_upstream_request();
    assert_eq!(captured1["geminiSettings"]["recitationPolicy"]["action"], "BLOCK");
    assert_eq!(captured1["geminiSettings"]["recitationPolicy"]["threshold"], "LOW");

    // Test 2: Request with NONE mode
    let req2 = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "tools": [/* ... */],
        "tool_choice": {"type": "none"},
        "max_tokens": 1024
    });
    let _response2 = send_claude_request(req2).await.unwrap();
    let captured2 = get_last_upstream_request();
    assert!(captured2.get("geminiSettings").is_some());

    // Test 3: Request without tools
    let req3 = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "max_tokens": 1024
    });
    let _response3 = send_claude_request(req3).await.unwrap();
    let captured3 = get_last_upstream_request();
    assert!(captured3.get("geminiSettings").is_some());

    // Test 4: Request without thinking
    let req4 = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "tools": [/* ... */],
        "max_tokens": 1024
    });
    let _response4 = send_claude_request(req4).await.unwrap();
    let captured4 = get_last_upstream_request();
    assert!(captured4.get("geminiSettings").is_some());
}
```

#### Scenario 6: Backward Compatibility Validation

**Goal**: Verify existing requests without tool_choice still work

**Test Case**:
```rust
#[tokio::test]
async fn test_backward_compatibility_no_tool_choice() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{
            "role": "user",
            "content": "Hello, world!"
        }],
        "tools": [{
            "type": "function",
            "function": {
                "name": "test_tool",
                "description": "Test",
                "parameters": {"type": "object", "properties": {}}
            }
        }],
        // No tool_choice field
        "max_tokens": 1024
    });

    let response = send_claude_request(request).await.unwrap();

    // Verify defaults to VALIDATED mode
    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["mode"],
        "VALIDATED"
    );

    // Verify no allowedFunctionNames
    assert!(captured_request["toolConfig"]["functionCallingConfig"]
        .get("allowedFunctionNames")
        .is_none());

    // Verify response successful
    assert!(response.is_ok());
}
```

#### Scenario 7: Error Handling Validation

**Goal**: Verify proper error handling for invalid configurations

**Test Cases**:
```rust
#[tokio::test]
async fn test_invalid_tool_choice_format() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "tools": [/* ... */],
        "tool_choice": {"type": "invalid_mode"},  // Invalid
        "max_tokens": 1024
    });

    let result = send_claude_request(request).await;

    // Should return 400 Bad Request
    assert!(result.is_err());
    let error = result.unwrap_err();
    assert_eq!(error.status(), 400);
    assert!(error.body().contains("invalid tool_choice"));
}

#[tokio::test]
async fn test_tool_forcing_nonexistent_tool() {
    let request = json!({
        "model": "claude-4.5-sonnet-thinking",
        "messages": [{"role": "user", "content": "Test"}],
        "tools": [{
            "type": "function",
            "function": {"name": "existing_tool", "description": "Test"}
        }],
        "tool_choice": {
            "type": "tool",
            "name": "nonexistent_tool"  // Not in tools array
        },
        "max_tokens": 1024
    });

    // Should pass-through to upstream (let backend validate)
    let _response = send_claude_request(request).await;

    let captured_request = get_last_upstream_request();
    assert_eq!(
        captured_request["toolConfig"]["functionCallingConfig"]["allowedFunctionNames"],
        json!(["nonexistent_tool"])
    );

    // Expect upstream error or empty tool response
}
```

---

## Acceptance Criteria

### AC-1: AUTO Mode End-to-End âœ…

**Given** a request with `tool_choice: {type: "auto"}`
**When** I send to Vertex AI
**Then** it should:
- âœ… Set `toolConfig.mode` to "AUTO" in upstream request
- âœ… Receive valid response from backend
- âœ… Model autonomously decides tool usage
- âœ… No errors or rejections

**Validation**: Scenario 1 test passes

### AC-2: ANY Mode End-to-End âœ…

**Given** a request with `tool_choice: {type: "any"}`
**When** I send to Vertex AI
**Then** it should:
- âœ… Set `toolConfig.mode` to "ANY" in upstream request
- âœ… Model calls at least one tool in response
- âœ… Receive valid response with tool_use block
- âœ… No errors or rejections

**Validation**: Scenario 2 test passes

### AC-3: NONE Mode End-to-End âœ…

**Given** a request with `tool_choice: {type: "none"}`
**When** I send to Vertex AI
**Then** it should:
- âœ… Set `toolConfig.mode` to "NONE" in upstream request
- âœ… Model does NOT call any tools
- âœ… Receive text-only response
- âœ… No errors or rejections

**Validation**: Scenario 3 test passes

### AC-4: Tool Forcing End-to-End âœ…

**Given** a request with `tool_choice: {type: "tool", name: "specific_tool"}`
**When** I send to Vertex AI
**Then** it should:
- âœ… Set `toolConfig.mode` to "VALIDATED"
- âœ… Set `allowedFunctionNames` to ["specific_tool"]
- âœ… Model calls ONLY the specified tool
- âœ… Receive valid response

**Validation**: Scenario 4 test passes

### AC-5: geminiSettings Always Present âœ…

**Given** any request configuration
**When** I send to Vertex AI
**Then** geminiSettings should:
- âœ… Always be present in upstream request
- âœ… Have action="BLOCK", threshold="LOW"
- âœ… Work with AUTO, ANY, NONE, VALIDATED modes
- âœ… Work with and without tools
- âœ… Work with and without thinking

**Validation**: Scenario 5 test passes

### AC-6: Backward Compatibility âœ…

**Given** a request without tool_choice field
**When** I send to Vertex AI
**Then** it should:
- âœ… Default to VALIDATED mode
- âœ… Not set allowedFunctionNames
- âœ… Work exactly as before
- âœ… No regressions

**Validation**: Scenario 6 test passes

### AC-7: Error Handling âœ…

**Given** invalid tool_choice configurations
**When** I send requests
**Then** it should:
- âœ… Return 400 for invalid tool_choice format
- âœ… Pass-through nonexistent tool names to upstream
- âœ… Handle upstream errors gracefully
- âœ… Provide clear error messages

**Validation**: Scenario 7 tests pass

### AC-8: Compliance Score 100% âœ…

**Given** all Phase 3 implementations complete
**When** I validate against Gap Analysis checklist
**Then** it should:
- âœ… Close all 8 gaps (from Gap Analysis)
- âœ… Match Antigravity baseline exactly
- âœ… Pass manual validation against RE spec
- âœ… Achieve 100% compliance score

**Validation**: Manual compliance checklist completion

### AC-9: Performance Validation âœ…

**Given** all new features implemented
**When** I measure request/response latency
**Then** it should:
- âœ… Add <100ms overhead for tool mode mapping
- âœ… Add <10ms for geminiSettings addition
- âœ… No degradation in existing request performance
- âœ… Maintain upstream API performance

**Validation**: Performance benchmarking

### AC-10: Integration Test Suite Complete âœ…

**Given** this story's test implementation
**When** I run the full integration test suite
**Then** it should:
- âœ… All 7 scenarios pass (Scenarios 1-7)
- âœ… All tests use live Vertex AI endpoint
- âœ… Tests cover all tool modes
- âœ… Tests cover all edge cases
- âœ… Tests run reliably (no flaky tests)

**Validation**: All integration tests passing

---

## Testing Strategy

### Integration Tests (7 scenarios)

**File**: `src-tauri/tests/tool_mode_integration_tests.rs`

```rust
//! Integration tests for tool mode functionality
//!
//! These tests use live Vertex AI endpoints and validate end-to-end behavior.
//!
//! Prerequisites:
//! - Valid Google OAuth token
//! - Vertex AI API enabled
//! - Test project configured

#[cfg(test)]
mod tool_mode_tests {
    use super::*;

    // Scenario 1: AUTO mode
    #[tokio::test]
    #[ignore] // Run with: cargo test --test tool_mode_integration_tests -- --ignored
    async fn test_e2e_tool_mode_auto() {
        // Implementation from Scenario 1 above
    }

    // Scenario 2: ANY mode
    #[tokio::test]
    #[ignore]
    async fn test_e2e_tool_mode_any() {
        // Implementation from Scenario 2 above
    }

    // Scenario 3: NONE mode
    #[tokio::test]
    #[ignore]
    async fn test_e2e_tool_mode_none() {
        // Implementation from Scenario 3 above
    }

    // Scenario 4: Tool forcing
    #[tokio::test]
    #[ignore]
    async fn test_e2e_tool_forcing() {
        // Implementation from Scenario 4 above
    }

    // Scenario 5: geminiSettings presence
    #[tokio::test]
    #[ignore]
    async fn test_gemini_settings_always_present() {
        // Implementation from Scenario 5 above
    }

    // Scenario 6: Backward compatibility
    #[tokio::test]
    #[ignore]
    async fn test_backward_compatibility_no_tool_choice() {
        // Implementation from Scenario 6 above
    }

    // Scenario 7: Error handling
    #[tokio::test]
    #[ignore]
    async fn test_invalid_tool_choice_format() {
        // Implementation from Scenario 7a above
    }

    #[tokio::test]
    #[ignore]
    async fn test_tool_forcing_nonexistent_tool() {
        // Implementation from Scenario 7b above
    }

    // Test helpers
    async fn send_claude_request(request: serde_json::Value) -> Result<serde_json::Value, Error> {
        // Send request through proxy to Vertex AI
        // ...
    }

    fn get_last_upstream_request() -> serde_json::Value {
        // Capture and return last upstream request
        // (requires request logging/capture infrastructure)
        // ...
    }
}
```

### Manual Testing Checklist

#### Tool Mode Validation
- [ ] **AUTO Mode**:
  - [ ] Send request with AUTO mode
  - [ ] Verify model decides when to use tools
  - [ ] Test with single tool and multiple tools
  - [ ] Verify toolConfig.mode = "AUTO" in logs

- [ ] **ANY Mode**:
  - [ ] Send request with ANY mode
  - [ ] Verify model calls at least one tool
  - [ ] Test with single tool (must call it)
  - [ ] Verify toolConfig.mode = "ANY" in logs

- [ ] **NONE Mode**:
  - [ ] Send request with NONE mode
  - [ ] Verify model returns text-only response
  - [ ] Verify no tool calls in response
  - [ ] Verify toolConfig.mode = "NONE" in logs

- [ ] **Tool Forcing**:
  - [ ] Send request forcing specific tool
  - [ ] Verify model calls ONLY that tool
  - [ ] Test with multiple tools available
  - [ ] Verify allowedFunctionNames in logs

#### Grounding Configuration Validation
- [ ] **geminiSettings Presence**:
  - [ ] Verify in AUTO mode request
  - [ ] Verify in ANY mode request
  - [ ] Verify in NONE mode request
  - [ ] Verify in VALIDATED mode request
  - [ ] Verify in request without tools
  - [ ] Verify in request without thinking

- [ ] **Recitation Policy Values**:
  - [ ] Verify action = "BLOCK"
  - [ ] Verify threshold = "LOW"
  - [ ] Verify format matches RE spec exactly

#### Compliance Validation
- [ ] **Gap Analysis Checklist** (from lines 3797-3805):
  - [x] Model provider info present (Stories 1-2)
  - [x] ideType: ANTIGRAVITY present (Story 3)
  - [x] JWT signatures validated (Story 5)
  - [x] Budget constraints logged (Story 6)
  - [x] Position violations logged (Story 7)
  - [ ] Tool modes configurable (Story 9) â† **This Story**
  - [ ] Grounding config present (Story 10) â† **This Story**
  - [x] Session metadata complete (Story 4)

- [ ] **Request Structure Comparison**:
  - [ ] Compare captured request with Antigravity baseline
  - [ ] Verify identical field ordering
  - [ ] Verify identical value formats
  - [ ] Verify no extra or missing fields

#### Performance Validation
- [ ] **Latency Measurement**:
  - [ ] Measure request mapping time (<100ms target)
  - [ ] Measure end-to-end latency (no degradation)
  - [ ] Test with various request sizes

- [ ] **Error Handling**:
  - [ ] Test invalid tool_choice format
  - [ ] Test nonexistent tool forcing
  - [ ] Test upstream API errors
  - [ ] Verify error messages are clear

#### Backward Compatibility
- [ ] **Existing Workflows**:
  - [ ] Test request without tool_choice
  - [ ] Test request without tools
  - [ ] Test request without thinking
  - [ ] Test minimal request
  - [ ] Verify all existing features work

---

## Definition of Done

### Code Quality
- [ ] âœ… All 7 integration test scenarios implemented
- [ ] âœ… All tests using live Vertex AI endpoint
- [ ] âœ… Test helper functions implemented
- [ ] âœ… Request capture infrastructure working
- [ ] âœ… Code follows Rust best practices
- [ ] âœ… No clippy warnings
- [ ] âœ… Code formatted with rustfmt

### Testing
- [ ] âœ… All 7 integration scenarios passing
- [ ] âœ… Manual testing checklist completed
- [ ] âœ… Performance benchmarking completed
- [ ] âœ… Error handling validated
- [ ] âœ… Backward compatibility confirmed

### Validation
- [ ] âœ… All 10 AC validated
- [ ] âœ… Gap Analysis checklist complete (8/8 gaps closed)
- [ ] âœ… Compliance score: **100%**
- [ ] âœ… Manual validation against RE spec successful
- [ ] âœ… Live Vertex AI endpoint testing successful

### Documentation
- [ ] âœ… Test scenarios documented
- [ ] âœ… Manual testing checklist completed
- [ ] âœ… Performance results documented
- [ ] âœ… Known issues/limitations documented (if any)
- [ ] âœ… Story marked as completed

### Epic Completion
- [ ] âœ… Phase 3 (Feature Parity) complete
- [ ] âœ… Epic-003 updated with Phase 3 completion
- [ ] âœ… Compliance score verified at 100%
- [ ] âœ… Ready for Phase 4 (Enhancement & Monitoring)

---

## Dependencies

### Upstream Dependencies (Must Be Complete)
- âœ… **Story-003-09**: Flexible Tool Configuration Modes (IMPLEMENTED)
- âŒ **Story-003-10**: Grounding Configuration (NOT IMPLEMENTED - BLOCKER)
- âš ï¸ **Story-003-01 to 003-08**: Implementation status UNKNOWN (needs verification)

**CRITICAL BLOCKER**: Story-003-10 geminiSettings implementation MUST be completed before Story-003-11 testing can proceed.

**Why**: This story validates implementations from Stories 009 and 010. Without Story-003-10, testing is incomplete and compliance cannot reach 100%.

### Downstream Dependencies
- **Story-003-12**: Compliance Monitoring Dashboard (will display test results)

**Impact**: Dashboard will show 100% compliance after this story.

### Infrastructure Dependencies
- **Live Vertex AI Access**: Valid OAuth token and enabled API
- **Request Logging**: Capture infrastructure for upstream requests
- **Test Environment**: Configured test project

---

## Risk Analysis

### Risk #1: Live Endpoint Testing Reliability
**Severity**: MEDIUM
**Probability**: MEDIUM
**Impact**: Flaky tests, unreliable validation

**Mitigation**:
- âœ… Use `#[ignore]` for integration tests (run separately)
- âœ… Implement retry logic for network issues
- âœ… Clear test isolation and cleanup
- âœ… Document prerequisites clearly

**Contingency**: If live testing unreliable, use request capture validation only.

### Risk #2: Backend Rejection of New Configurations
**Severity**: HIGH
**Probability**: LOW
**Impact**: Requests rejected, features don't work

**Mitigation**:
- âœ… Follow Gap Analysis exact specifications
- âœ… Manual validation before automated testing
- âœ… Start with VALIDATED mode (known working)
- âœ… Test one mode at a time

**Contingency**: If mode rejected, investigate error and adjust implementation.

### Risk #3: Performance Degradation
**Severity**: LOW
**Probability**: LOW
**Impact**: Increased latency from new features

**Mitigation**:
- âœ… Benchmark before and after implementation
- âœ… Target <100ms overhead
- âœ… Optimize hot paths
- âœ… Profile if needed

**Measurement**: AC-9 performance validation.

### Risk #4: Incomplete Test Coverage
**Severity**: MEDIUM
**Probability**: LOW
**Impact**: Bugs in production, compliance issues

**Mitigation**:
- âœ… Comprehensive test scenarios (7 scenarios)
- âœ… Manual testing checklist
- âœ… Gap Analysis checklist validation
- âœ… Multiple validation layers

**Contingency**: Add additional tests if gaps found during manual testing.

---

## Implementation Notes

### Testing Infrastructure Requirements

**Request Capture**:
- Need infrastructure to capture upstream requests
- Options: Logging middleware, test spy, mock upstream
- Requirement: Access to full JSON request sent to Vertex AI

**Test Environment**:
- Separate test project/account recommended
- Valid OAuth token with sufficient quota
- Vertex AI API enabled

**Test Isolation**:
- Each test should be independent
- No shared state between tests
- Clean setup/teardown

### Manual Testing Approach

**Validation Sequence**:
1. Start with VALIDATED mode (baseline)
2. Test AUTO mode next (least restrictive)
3. Test ANY mode (forced tool usage)
4. Test NONE mode (disabled tools)
5. Test tool forcing last (most specific)

**For Each Mode**:
1. Capture and inspect upstream request
2. Verify mode and allowedFunctionNames
3. Verify geminiSettings present
4. Check backend response
5. Validate response format
6. Document any issues

### Performance Benchmarking

**Metrics to Collect**:
- Request mapping time (target: <100ms)
- End-to-end latency (no degradation)
- Memory usage (minimal increase)
- CPU usage (minimal increase)

**Baseline**: Measure before implementing Stories 009-010
**Target**: <10% overhead from new features

---

## File Impact Analysis

### New Files

| File | Lines | Description |
|------|-------|-------------|
| `tool_mode_integration_tests.rs` | ~500 | 7 integration test scenarios |

### Modified Files

| File | Lines Changed | Change Type | Description |
|------|---------------|-------------|-------------|
| `Cargo.toml` | +3 | Addition | Test dependencies if needed |

**Total Changes**:
- **Test Code**: ~500 lines
- **Infrastructure**: Minimal (test helpers)

---

## Change Log

| Date | Change | Author |
|------|--------|--------|
| 2026-01-10 | Story created with comprehensive test scenarios | BMad Master |

---

## References

### Dependency Stories
- **Story-003-09**: `docs/stories/Story-003-09-flexible-tool-configuration-modes.md`
  - Tool mode implementation and unit tests
- **Story-003-10**: `docs/stories/Story-003-10-grounding-configuration.md`
  - geminiSettings implementation and unit tests

### Gap Analysis
- **Document**: `docs/comparison/claude/claude-4-5-sonnet/current-implementation-thinking.md`
  - Lines 3730-3806: Testing Strategy
  - Lines 3797-3805: Compliance checklist

### Epic Definition
- **Epic-003**: `docs/epics/Epic-003-Claude-4.5-Sonnet-Thinking-Compliance.md`
  - Phase 3 Success Criteria
  - Epic-level Definition of Done
